{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c30c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baby GPT training notebook (single-file script / runnable in a Jupyter cell or as a .py)\n",
    "# Filename: baby_gpt_notebook.py\n",
    "# Purpose: Train a tiny decoder-only transformer (baby GPT) on sample soccer text.\n",
    "# Notes:\n",
    "# - Designed to run on CPU or GPU. If you have a GPU, PyTorch will use it automatically.\n",
    "# - Installs required packages when run in a fresh environment.\n",
    "# - Uses HuggingFace tokenizer for convenience.\n",
    "\n",
    "# %%\n",
    "# 1) Install dependencies (run once)\n",
    "# If running in Colab or a fresh env, uncomment the following pip commands.\n",
    "# In a local environment you might already have these packages installed.\n",
    "\n",
    "# !pip install -q transformers datasets torch tqdm\n",
    "\n",
    "# %%\n",
    "# 2) Imports\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import GPT2TokenizerFast\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# %%\n",
    "# 3) Tiny dataset (we'll use the 1000-word soccer text + a few synthetic match lines)\n",
    "# You can replace `raw_text` with any large text file for better results.\n",
    "raw_text = '''\n",
    "Soccer, known as football outside North America, is the world’s most popular sport — a\n",
    "simple game with rich complexity. At its core, soccer is played between two teams of\n",
    "eleven players on a rectangular pitch with the objective of moving a spherical ball\n",
    "into the opponent’s net more times than the opponent during a fixed interval. Yet a\n",
    "single match contains dozens of interacting systems: individual skill, team tactics,\n",
    "coaches’ game plans, referee decisions, and the unpredictable variables of weather,\n",
    "pitch condition, and fan atmosphere. These layers make soccer both an accessible\n",
    "pastime and a deep subject for analysis.\n",
    "\n",
    "From grassroots to elite levels, soccer is shaped by its rules and by the continuous\n",
    "evolution of tactics. The Laws of the Game, maintained by the International Football\n",
    "Association Board (IFAB), provide the framework — fouls, offside, substitutions, and\n",
    "restart procedures — while coaches constantly innovate within that framework. Over the\n",
    "past decades, tactical trends have come and gone: from rigid formations to fluid,\n",
    "positionless systems; from deep defensive blocks to intense, high-pressing attacks\n",
    "that aim to win the ball high up the pitch. Modern teams often blend approaches,\n",
    "shifting shapes dynamically depending on game state, opponent, and available\n",
    "personnel.\n",
    "\n",
    "Players are the sport’s primary storytellers. Technical skills such as first touch,\n",
    "passing range, dribbling, and finishing create moments of magic, while physical\n",
    "attributes — speed, strength, and stamina — determine whether a player can execute\n",
    "repeated high-intensity actions across 90 minutes. Yet soccer prizes decision-making\n",
    "and spatial intelligence arguably above pure athleticism. The elite practitioners\n",
    "read the game: they anticipate runs, manage tempo, and choose when to accelerate or\n",
    "conserve energy. Young players are typically developed through a mix of deliberate\n",
    "practice and game intelligence training, where coaches encourage pattern recognition\n",
    "and variability of practice rather than rote repetition.\n",
    "\n",
    "Clubs and national teams operate inside an ecosystem shaped by youth academies,\n",
    "scouting networks, sports science, and data analytics. Academies are talent pipelines,\n",
    "using structured training programs to develop technical foundations and tactical\n",
    "understanding. Scouting extends this pipeline, combining in-person observation with\n",
    "video and data-driven scouting to find undervalued prospects. Sports science and\n",
    "medical teams optimize player load, recovery, and nutrition to reduce injury risk\n",
    "and maximize performance. In recent years, data analytics has exploded in influence:\n",
    "tracking data, event logs, and advanced metrics are used to evaluate players, plan\n",
    "tactics, and inform recruitment. Analysts convert raw event streams — passes, shots,\n",
    "tackles, positional coordinates — into actionable insights that coaches use to\n",
    "gain marginal advantages.\n",
    "\n",
    "Competitions are central to soccer’s global culture. Domestic leagues provide weekly\n",
    "drama across seasons, continental competitions like the UEFA Champions League\n",
    "assemble elite clubs from different countries, and international tournaments such as\n",
    "the FIFA World Cup capture the world’s attention periodically. Each competition has\n",
    "different incentives: domestic leagues prize consistency over 38–40 matches, cup\n",
    "competitions reward knockout efficiency, and international tournaments reward\n",
    "short-term peak performance. Fans scaffold these competitions with rituals — chants,\n",
    "scarves, matchday foods — creating tribal identities that amplify the stakes and\n",
    "intensity.\n",
    "\n",
    "The economics of soccer are complex and increasingly globalized. Broadcasting rights,\n",
    "sponsorships, and commercial partnerships bring revenue that determines clubs’ transfer\n",
    "budgets and wage structures. While elite clubs can leverage vast revenue streams to\n",
    "assemble world-class squads, economic disparity exists between top-tier clubs and\n",
    "smaller clubs whose survival often depends on player development and smart trading.\n",
    "Financial regulations, such as spending rules and licensing, aim to promote stability\n",
    "but are not universally enforced in the same way across leagues.\n",
    "\n",
    "Technology has also reshaped how the game is played and adjudicated. Video Assistant\n",
    "Referee (VAR) systems aim to reduce clear errors for goals, penalties, red cards, and\n",
    "identity mistakes. Wearable GPS and inertial measurement systems provide teams with\n",
    "fine-grained load data. Broadcasting innovations and camera systems enhance the\n",
    "viewer experience and make previously hidden tactical elements visible. Meanwhile,\n",
    "fan engagement has expanded into social media, fantasy sports, and interactive\n",
    "viewing experiences that further entrench soccer’s cultural footprint.\n",
    "\n",
    "Soccer’s social role cannot be overstated. It fosters community cohesion, provides\n",
    "pathways for social mobility, and serves as a platform for social and political\n",
    "expression. From local youth clubs to global campaigns for equality and anti-racism,\n",
    "soccer often intersects with broader societal issues. Its global reach makes it an\n",
    "effective vehicle for cultural exchange, yet it also concentrates power and attention\n",
    "among a limited number of clubs and players.\n",
    "\n",
    "For researchers and engineers working with soccer data, the richness of the sport\n",
    "translates into many modeling opportunities: event prediction, player valuation,\n",
    "tactical pattern discovery, and generative tasks like simulating plausible match\n",
    "narratives or commentary. The success of those models depends on data quality,\n",
    "representativeness, and careful feature engineering. Event-level data and tracking\n",
    "data enable complementary analyses: event data is ideal for understanding actions and\n",
    "outcomes, while tracking data captures movement and space control.\n",
    "\n",
    "In short, soccer is a deceptively simple game whose surface belies a deep,\n",
    "interconnected system. Whether you love the sport for the drama of a late winner,\n",
    "the elegance of a well-worked team move, or the intellectual puzzle of tactical\n",
    "analysis, soccer offers experiences that are simultaneously universal and endlessly\n",
    "detailed.\n",
    "'''\n",
    "\n",
    "# Add some structured match lines to increase pattern variety\n",
    "sample_matches = [\n",
    "    \"2024-08-10 — Arsenal 2-1 Manchester City; possession 54-46; shots 12-9.\",\n",
    "    \"2024-08-10 — Manchester United 1-1 Chelsea; possession 49-51; shots 10-11.\",\n",
    "    \"2024-08-11 — Real Madrid 3-2 Barcelona; possession 60-40; shots 15-13.\",\n",
    "    \"2024-08-12 — Juventus 0-0 AC Milan; possession 47-53; shots 8-7.\",\n",
    "]\n",
    "\n",
    "full_text = raw_text + \"\\n\\n\" + \"\\n\".join(sample_matches)\n",
    "\n",
    "# %%\n",
    "# 4) Tokenizer (GPT-2 tokenizer is convenient; using small vocab is fine)\n",
    "# If you want to train your own tokenizer on a massive corpus, replace this with\n",
    "# training a new ByteLevelBPETokenizer. For this tiny demo we use prebuilt GPT-2 vocab.\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "# Add a special token for separation if desired\n",
    "special_tokens = {\"pad_token\": \"<|pad|>\"}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# Tokenize the full text\n",
    "enc = tokenizer(full_text)\n",
    "input_ids = torch.tensor(enc[\"input_ids\"], dtype=torch.long)\n",
    "\n",
    "print(f\"Tokenized dataset length (tokens): {input_ids.size(0)}\")\n",
    "\n",
    "# %%\n",
    "# 5) Prepare dataset with sliding windows (typical for language modeling)\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokens, block_size=128):\n",
    "        self.tokens = tokens\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        # number of training examples from sliding window\n",
    "        return max(0, self.tokens.size(0) - self.block_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.tokens[idx : idx + self.block_size]\n",
    "        y = self.tokens[idx + 1 : idx + 1 + self.block_size]\n",
    "        return x, y\n",
    "\n",
    "block_size = 128\n",
    "dataset = TextDataset(input_ids, block_size=block_size)\n",
    "loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# %%\n",
    "# 6) Define a tiny decoder-only transformer (baby GPT)\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, n_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % n_heads == 0\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = embed_dim // n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=False)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        # causal mask will be created dynamically in forward\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.qkv(x)  # (B, T, 3*C)\n",
    "        qkv = qkv.reshape(B, T, 3, self.n_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # each is (B, n_heads, T, head_dim)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * self.scale  # (B, n_heads, T, T)\n",
    "        # causal mask: set attention for j>i to -inf\n",
    "        mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()\n",
    "        att = att.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "\n",
    "        att = torch.softmax(att, dim=-1)\n",
    "        out = att @ v  # (B, n_heads, T, head_dim)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.proj(out)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ff_mult=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * ff_mult),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * ff_mult, embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, embed_dim, n_heads):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = CausalSelfAttention(embed_dim, n_heads)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = FeedForward(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class BabyGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size=128, n_layers=4, n_heads=4, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = nn.Embedding(block_size, embed_dim)\n",
    "        self.blocks = nn.ModuleList([Block(embed_dim, n_heads) for _ in range(n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "\n",
    "        # weight tying\n",
    "        self.head.weight = self.tok_emb.weight\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.block_size, \"Sequence length exceeds block size\"\n",
    "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)\n",
    "        x = self.tok_emb(idx) + self.pos_emb(pos)\n",
    "        for b in self.blocks:\n",
    "            x = b(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "# %%\n",
    "# 7) Create model, optimizer, device\n",
    "vocab_size = len(tokenizer)\n",
    "model = BabyGPT(vocab_size=vocab_size, block_size=block_size, n_layers=4, n_heads=4, embed_dim=128)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "\n",
    "# %%\n",
    "# 8) Training loop (small number of steps for demo)\n",
    "num_epochs = 20\n",
    "log_interval = 10\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch}\")\n",
    "    running_loss = 0.0\n",
    "    for step, (x, y) in enumerate(pbar):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        logits = model(x)  # (B, T, V)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if (step + 1) % log_interval == 0:\n",
    "            pbar.set_postfix({\"loss\": running_loss / log_interval})\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Generate some text at the end of each epoch to monitor progress\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # prompt (tokenize a short seed)\n",
    "        seed = \"Soccer is\"\n",
    "        seed_ids = torch.tensor(tokenizer(seed)[\"input_ids\"], dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "        # generate autoregressively\n",
    "        for _ in range(60):\n",
    "            if seed_ids.size(1) > block_size:\n",
    "                seed_ids = seed_ids[:, -block_size:]\n",
    "            logits = model(seed_ids)\n",
    "            probs = F.softmax(logits[:, -1, :], dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)\n",
    "            seed_ids = torch.cat([seed_ids, next_id], dim=1)\n",
    "\n",
    "        gen = tokenizer.decode(seed_ids.squeeze().tolist())\n",
    "        print(f\"\\n=== Sample (epoch {epoch}) ===\\n{gen}\\n\")\n",
    "    model.train()\n",
    "\n",
    "# %%\n",
    "# 9) Save the model and tokenizer\n",
    "out_dir = Path(\"baby_gpt_checkpoints\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "save_path = out_dir / \"baby_gpt.pth\"\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"tokenizer\": tokenizer.get_vocab(),\n",
    "}, save_path)\n",
    "print(f\"Saved checkpoint to {save_path}\")\n",
    "\n",
    "# Also save tokenizer in HF format for reuse\n",
    "tokenizer.save_pretrained(out_dir)\n",
    "\n",
    "# %%\n",
    "# 10) Notes and next steps\n",
    "# - This is a tiny model trained on very little data; results are mainly illustrative.\n",
    "# - For better results:\n",
    "#   * Train on much more text (millions of tokens)\n",
    "#   * Increase model size (layers, heads, embed_dim)\n",
    "#   * Use learning rate schedules (cosine, warmup)\n",
    "#   * Use mixed precision (AMP) and larger batch sizes on GPU\n",
    "#   * Consider HuggingFace `Trainer` or DeepSpeed/FairScale for scaling\n",
    "# - If you'd like, I can convert this to a full Jupyter .ipynb, add Colab-ready cells,\n",
    "#   or make a HuggingFace `datasets` + `Trainer` version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb00b794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
