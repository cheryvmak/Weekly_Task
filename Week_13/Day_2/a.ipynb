{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f471493b",
   "metadata": {},
   "source": [
    "####  Data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ba1dae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe6634a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/cheryvmak/Dataset-Repo/refs/heads/main/Data_files/home_loan_train.csv'\n",
    "train_data = pd.read_csv(url)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00284882",
   "metadata": {},
   "source": [
    "Preliminary Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9724d078",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# copy and Read in data\n",
    "df1 = train_data.copy()\n",
    "\n",
    "# Inspect dataset\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f141b00",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Lets check our data\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcd6343",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Lets take a snap shot of our data\n",
    "print(f\"Rows: {df1.shape[0]:,}\")\n",
    "print(f\"Columns: {df1.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2125e5d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Just an identifier â€” adds no predictive value.\n",
    "df1 = df1.drop(columns=['Loan_ID'])\n",
    "df1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421458d6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96931eb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Lets check our datatypes\n",
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a752049",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#df1.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4055b52",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#df1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf7122e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# def clean_loan_data(df1):\n",
    "#     # fill missing values\n",
    "#     for col in df1.select_dtypes(include='object'):\n",
    "#         df1[col].fillna(df1[col].mode()[0], inplace=True)\n",
    "#     for col in df1.select_dtypes(include=['int64', 'float64']):\n",
    "#         df1[col].fillna(df1[col].median(), inplace=True)\n",
    "\n",
    "#     # type conversions\n",
    "#     for col in df1.select_dtypes(include='object'):\n",
    "#         df1[col] = df1[col].astype('category')\n",
    "#         df1[col] = df1[col].str.strip().str.lower()\n",
    "\n",
    "\n",
    "#     return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ab3a08",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# df1_cleaned = clean_loan_data(df1)\n",
    "# df1_cleaned.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553ac925",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# # Replace '3+' with a number\n",
    "# df1_cleaned['Dependents'] = df1_cleaned['Dependents'].replace('3+', '3')\n",
    "# # Convert the column to numeric\n",
    "# df1_cleaned['Dependents'] = df1_cleaned['Dependents'].astype(int)\n",
    "\n",
    "\n",
    "# df1_cleaned['Loan_Amount_Term'] = df1_cleaned['Loan_Amount_Term'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d36b97",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# def cap_outliers(df1_cleaned):\n",
    "#     \"\"\"\n",
    "#     Caps outliers for specific features based on their type and business logic.\n",
    "#     Reports number and percentage of outliers before capping.\n",
    "#     \"\"\"\n",
    "\n",
    "#     df1_cleaned = df1_cleaned.copy()\n",
    "\n",
    "#     # Function to count outliers using IQR rule\n",
    "#     def count_outliers(series):\n",
    "#         Q1 = series.quantile(0.25)\n",
    "#         Q3 = series.quantile(0.75)\n",
    "#         IQR = Q3 - Q1\n",
    "#         lower_bound = Q1 - 1.5 * IQR\n",
    "#         upper_bound = Q3 + 1.5 * IQR\n",
    "#         outliers = series[(series < lower_bound) | (series > upper_bound)]\n",
    "#         return len(outliers), 100 * len(outliers) / len(series)\n",
    "\n",
    "#     # Percentile-based capping function\n",
    "#     def cap_percentile(series, lower=0.01, upper=0.99):\n",
    "#         lower_cap = series.quantile(lower)\n",
    "#         upper_cap = series.quantile(upper)\n",
    "#         return np.clip(series, lower_cap, upper_cap)\n",
    "\n",
    "#     # IQR-based capping function\n",
    "#     def cap_iqr(series):\n",
    "#         Q1 = series.quantile(0.25)\n",
    "#         Q3 = series.quantile(0.75)\n",
    "#         IQR = Q3 - Q1\n",
    "#         lower_bound = Q1 - 1.5 * IQR\n",
    "#         upper_bound = Q3 + 1.5 * IQR\n",
    "#         return np.clip(series, lower_bound, upper_bound)\n",
    "\n",
    "#     # ---- Apply Feature-Specific Logic ----\n",
    "#     numeric_features = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term']\n",
    "\n",
    "#     for feature in numeric_features:\n",
    "#         if feature in df1_cleaned.columns:\n",
    "#             n_outliers, pct_outliers = count_outliers(df1_cleaned[feature])\n",
    "#             print(f\"\\nFeature: {feature}\")\n",
    "#             print(f\" Number of outliers: {n_outliers}\")\n",
    "#             print(f\" Outlier percentage: {pct_outliers:.2f}%\")\n",
    "\n",
    "#             if feature in ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']:\n",
    "#                 df1_cleaned[feature] = cap_percentile(df1_cleaned[feature])\n",
    "#                 print(f\"Applied percentile capping (1stâ€“99th percentile).\")\n",
    "#             elif feature == 'Loan_Amount_Term':\n",
    "#                 df1_cleaned[feature] = cap_iqr(df1_cleaned[feature])\n",
    "#                 print(f\"Applied IQR-based capping.\")\n",
    "    \n",
    "#     # Skip categorical/binary\n",
    "#     skip_features = ['Dependents', 'Credit_History']\n",
    "#     print(f\"\\nSkipped features (categorical/binary): {skip_features}\")\n",
    "#     print(\"\\nOutlier capping completed successfully.\\n\")\n",
    "\n",
    "#     return df1_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3513fc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# df_capped = cap_outliers(df1_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99600e01",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# df1_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a17ac8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# df_capped.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870330c0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import PowerTransformer\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# def transform_features(df1_cleaned, columns, method='yeo-johnson'):\n",
    "#     \"\"\"\n",
    "#     Applies log, Box-Cox, or Yeo-Johnson transformation to specified columns.\n",
    "#     Handles zero or negative values safely and skips near-binary variables.\n",
    "\n",
    "#     Parameters:\n",
    "#     - df1_transformed: pandas DataFrame\n",
    "#     - columns: list of columns to transform\n",
    "#     - method: 'log', 'boxcox', or 'yeo-johnson'\n",
    "    \n",
    "#     Returns:\n",
    "#     - Transformed DataFrame\n",
    "#     \"\"\"\n",
    "#     df1_cleaned = df1_cleaned.copy()\n",
    "\n",
    "#     for col in columns:\n",
    "#         if col not in df1_cleaned.columns:\n",
    "#             print(f\"Column '{col}' not found. Skipping.\")\n",
    "#             continue\n",
    "\n",
    "#         unique_vals = df1_cleaned[col].nunique()\n",
    "\n",
    "#         # Skip near-binary or categorical numeric features\n",
    "#         if unique_vals <= 4:\n",
    "#             print(f\"Skipping transformation for '{col}' (only {unique_vals} unique values).\")\n",
    "#             continue\n",
    "\n",
    "#         series = df1_cleaned[col]  # ignore NaNs temporarily\n",
    "#         before_skew = series.skew()\n",
    "\n",
    "#         # ---- LOG TRANSFORMATION ----\n",
    "#         if method == 'log':\n",
    "#             if (series <= 0).any():\n",
    "#                 shift = abs(series.min()) + 1\n",
    "#                 df1_cleaned[col] = np.log1p(df1_cleaned[col] + shift)\n",
    "#                 print(f\"Applied log1p (shift={shift:.2f}) to '{col}'\")\n",
    "#             else:\n",
    "#                 df1_cleaned[col] = np.log1p(df1_cleaned[col])\n",
    "#                 print(f\"Applied log1p to '{col}'\")\n",
    "\n",
    "#         # ---- BOX-COX or YEOâ€“JOHNSON ----\n",
    "#         elif method in ['boxcox', 'yeo-johnson']:\n",
    "#             transformer = PowerTransformer(method=method, standardize=False)\n",
    "#             reshaped = df1_cleaned[col].values.reshape(-1, 1)\n",
    "\n",
    "#             # Skip Boxâ€“Cox if non-positive values exist\n",
    "#             if method == 'boxcox' and (df1_cleaned[col] <= 0).any():\n",
    "#                 print(f\"Skipping Boxâ€“Cox for '{col}' (contains non-positive values).\")\n",
    "#                 continue\n",
    "\n",
    "#             try:\n",
    "#                 df1_cleaned[col] = transformer.fit_transform(reshaped)\n",
    "#                 after_skew = df1_cleaned[col].skew()\n",
    "#                 print(f\"Applied {method.title()} to '{col}' | Before Skew: {before_skew:.4f}, After: {after_skew:.4f}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Could not apply {method.title()} to '{col}': {e}\")\n",
    "\n",
    "#         else:\n",
    "#             print(f\"Unknown method '{method}'. Choose from ['log', 'boxcox', 'yeo-johnson']\")\n",
    "\n",
    "#     print(\"\\nTransformation completed successfully.\\n\")\n",
    "#     return df1_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8635e63f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# check_unique_values(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f540f75f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# # Label encoding based on logical binary mapping\n",
    "# df1_cleaned['Gender'] = df1_cleaned['Gender'].map({'male': 1, 'female': 0})\n",
    "# df1_cleaned['Married'] = df1_cleaned['Married'].map({'yes': 1, 'no': 0})\n",
    "# df1_cleaned['Education'] = df1_cleaned['Education'].map({'graduate': 1, 'not graduate': 0})\n",
    "# df1_cleaned['Self_Employed'] = df1_cleaned['Self_Employed'].map({'yes': 1, 'no': 0})\n",
    "# df1_cleaned['Loan_Status'] = df1_cleaned['Loan_Status'].map({'y': 1, 'n': 0})\n",
    "\n",
    "# # One-hot encode Property_Area\n",
    "# df1_cleaned = pd.get_dummies(df1_cleaned, columns=['Property_Area'], prefix='Property', drop_first=False, dtype=int)\n",
    "\n",
    "# df1_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6829c1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# df1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99082962",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# encoded_property_cols = [col for col in df1_cleaned.columns if col.startswith('Property_')]\n",
    "\n",
    "# for col in categorical_features:\n",
    "#     if col != 'Property_Area':\n",
    "#         ct = pd.crosstab(df1_cleaned[col], df1_cleaned['Loan_Status'], normalize='index') * 100\n",
    "#         print(f\"\\n{col} vs Loan Status:\\n\", ct.round(1))\n",
    "#     else:\n",
    "#         for prop_col in encoded_property_cols:\n",
    "#             ct = pd.crosstab(df1_cleaned[prop_col], df1_cleaned['Loan_Status'], normalize='index') * 100\n",
    "#             print(f\"\\n{prop_col} vs Loan Status:\\n\", ct.round(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a210e0b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# def plot_feature_relationships(df1, target='Loan_Status', num_features=None):\n",
    "#     \"\"\"\n",
    "#     Plots scatter plots for numerical features vs target variable.\n",
    "#     Works well when target is binary (0/1).\n",
    "#     \"\"\"\n",
    "    \n",
    "#     if num_features is None:\n",
    "#         num_features = df1.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    \n",
    "#     # Ensure target exists\n",
    "#     if target not in df1.columns:\n",
    "#         raise ValueError(f\"Target column '{target}' not found in DataFrame.\")\n",
    "    \n",
    "#     # Drop the target from the feature list\n",
    "#     if target in num_features:\n",
    "#         num_features.remove(target)\n",
    "    \n",
    "#     print(f\"ðŸ“Š Generating scatter plots for {len(num_features)} numerical features...\\n\")\n",
    "\n",
    "#     for col in num_features:\n",
    "#         plt.figure(figsize=(6, 4))\n",
    "#         #sns.scatterplot(data=df1_cleaned, x=col, y=target, alpha=0.6)\n",
    "#         plt.figure(figsize=(8, 5))\n",
    "#         sns.regplot(data=df1, x=col, y=target, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
    "#         plt.title(f\"{col} vs {target}\")\n",
    "#         plt.xlabel(col)\n",
    "#         plt.ylabel(target)\n",
    "#         plt.grid(alpha=0.3)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45254715",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# plot_feature_relationships(df1_cleaned, target='Loan_Status', num_features=numerical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbdf23c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# # Correlation heatmap\n",
    "# plt.figure(figsize=(8,6))\n",
    "# sns.heatmap(df1_cleaned[numerical_features + ['Loan_Status']].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "# plt.title(\"Feature Correlation with Loan Status\")\n",
    "# plt.show()\n",
    "\n",
    "# # Boxplot for LoanAmount vs Loan_Status\n",
    "# plt.figure(figsize=(6,4))\n",
    "# sns.boxplot(x='Loan_Status', y='LoanAmount', data=df1_cleaned)\n",
    "# plt.title(\"LoanAmount Distribution by Loan Status\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0e3c18",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#trivariate_plot(df1, 'Education', 'ApplicantIncome', 'Loan_Status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb40506",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# trivariate_plot(df1, 'Property_semiurban', 'LoanAmount', 'Loan_Status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1decd38d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#trivariate_plot(df1, 'Property_urban', 'LoanAmount', 'Loan_Status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975094be",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# trivariate_plot(df1, 'Property_rural', 'LoanAmount', 'Loan_Status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f42459b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#df1_transformed = transform_features(df1_cleaned, numerical_features, method='yeo-johnson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11de20b5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# transform_features = ['ApplicantIncome', 'CoapplicantIncome', \n",
    "#                 'LoanAmount', 'Loan_Amount_Term']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0b8327",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# df1_transformed[transform_features].skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc4d063",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def univariate_numerical_eda(df1, column):\n",
    "    \"\"\"\n",
    "    Performs univariate EDA on a single numerical column.\n",
    "    Displays summary statistics, skewness, kurtosis, histogram, KDE, boxplot, \n",
    "    and intelligent suggestions for skew/kurtosis handling (for ML preprocessing).\n",
    "    \"\"\"\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    print(f\"\\nFeature: {column}\")\n",
    "    print(\"=\"*45)\n",
    "    print(df1[column].describe().to_frame())\n",
    "\n",
    "    # --- Summary ---\n",
    "    total = len(df1[column])\n",
    "    missing = df1[column].isna().sum()\n",
    "    unique = df1[column].nunique()\n",
    "    \n",
    "\n",
    "  \n",
    "    print(f\"Missing values: {missing} ({(missing/total)*100:.2f}%)\")\n",
    "    print(f\"Unique categories: {unique}\")\n",
    "   \n",
    "    \n",
    "    skew = df1[column].skew()\n",
    "    kurt = df1[column].kurt()\n",
    "    \n",
    "    print(f\"\\nSkewness: {skew:.3f}\")\n",
    "    print(f\"Kurtosis: {kurt:.3f}\")\n",
    "\n",
    "     # ====== INTERPRETATION (Stricter ML-based) ======\n",
    "\n",
    "    # --- Skewness interpretation ---\n",
    "    \n",
    "    if abs(skew) > 0.7:\n",
    "        skew_status = \"high\"\n",
    "        print(f\"\\n{column} is strongly skewed. Consider log, Box-Cox, or Yeo-Johnson transformation.\")\n",
    "    elif abs(skew) > 0.3:\n",
    "        skew_status = \"moderate\"\n",
    "        print(f\"{column} is mildly skewed. A mild transformation (âˆš or cube root) may help.\")\n",
    "    else:\n",
    "        skew_status = \"normal\"\n",
    "        print(f\"{column} is fairly symmetric â€” no transformation likely needed.\")\n",
    "\n",
    "    # --- Kurtosis interpretation ---\n",
    "    if abs(kurt) > 1.0:\n",
    "        kurt_status = \"high\"\n",
    "        print(f\"{column} shows heavy tails (leptokurtic). Consider robust scaling or outlier treatment.\")\n",
    "    elif abs(kurt) > 0.5:\n",
    "        kurt_status = \"moderate\"\n",
    "        print(f\"{column} has mildly heavy tails. Light transformation or scaling may help.\")\n",
    "    else:\n",
    "        kurt_status = \"normal\"\n",
    "        print(f\"{column} has fairly normal kurtosis â€” suitable for most ML models.\")\n",
    "\n",
    "    # ====== COMBINED LOGIC ======\n",
    "    if (skew_status in [\"high\", \"moderate\"]) and kurt_status == \"normal\":\n",
    "        print(\"\\n You can focus on correcting the skew only (e.g., log-transform), \"\n",
    "              \"no need for outlier-heavy corrections.\")\n",
    "    elif (skew_status in [\"high\", \"moderate\"]) and (kurt_status in [\"high\", \"moderate\"]):\n",
    "        print(\"\\n The variable is both skewed and heavy-tailed. Consider both transformation \"\n",
    "              \"and outlier treatment.\")\n",
    "    elif skew_status == \"normal\" and kurt_status == \"normal\":\n",
    "        print(\"\\n The variable is approximately normal â€” no transformation needed.\")\n",
    "\n",
    "    # ====== PLOTS ======\n",
    "    plt.figure(figsize=(12,4))\n",
    "\n",
    "    # Histogram + KDE\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.histplot(df1[column], kde=True, bins=30, color='salmon')\n",
    "    plt.title(f'\\nDistribution of {column}', fontsize=13)\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Boxplot\n",
    "    plt.subplot(1,2,2)\n",
    "    sns.boxplot(x=df1[column], color='seagreen')\n",
    "    plt.title(f'\\nBoxplot of {column}', fontsize=13)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad831a6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for col in numerical_features:\n",
    "    univariate_numerical_eda(df1, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ef1faa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def univariate_categorical_eda(df1, column):\n",
    "    \"\"\"\n",
    "    Performs univariate EDA on a single categorical column.\n",
    "    Displays frequency distribution, proportion, missing values,\n",
    "    and visualizations (bar plot + pie chart).\n",
    "    Provides ML preprocessing suggestions.\n",
    "    \"\"\"\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "\n",
    "    print(f\"\\nFeature: {column}\")\n",
    "    print(\"=\"*45)\n",
    "\n",
    "    # --- Summary ---\n",
    "    total = len(df1[column])\n",
    "    missing = df1[column].isna().sum()\n",
    "    unique = df1[column].nunique()\n",
    "    mode_val = df1[column].mode()[0] if unique > 0 else None\n",
    "\n",
    "    print(f\"Total observations: {total}\")\n",
    "    print(f\"Missing values: {missing} ({(missing/total)*100:.2f}%)\")\n",
    "    print(f\"Unique categories: {unique}\")\n",
    "    print(f\"Most frequent category: {mode_val}\")\n",
    "\n",
    "    # --- Frequency Table ---\n",
    "    freq = df1[column].value_counts(dropna=False)\n",
    "    perc = df1[column].value_counts(normalize=True, dropna=False) * 100\n",
    "    summary = pd.DataFrame({'Count': freq, 'Percentage': perc.round(2)})\n",
    "    print(\"\\nCategory Distribution:\")\n",
    "    print(summary)\n",
    "\n",
    "    # --- Visualization ---\n",
    "    plt.figure(figsize=(12,4))\n",
    "\n",
    "    # Bar Plot\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.countplot(x=df1[column], palette='Set2', order=freq.index,color='teal')\n",
    "    plt.title(f\"Frequency of {column}\", fontsize=13)\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "    \n",
    "\n",
    "    # Pie Chart\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.pie(freq, labels=freq.index, autopct='%1.1f%%', startangle=90)\n",
    "    plt.title(f\"Proportion of {column}\", fontsize=13)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- ML Preprocessing Suggestions ---\n",
    "    if unique == 2:\n",
    "        print(f\" {column} is binary â€” suitable for Label Encoding (0/1).\")\n",
    "    elif 2 < unique <= 10:\n",
    "        print(f\" {column} has moderate categories â€” use One-Hot Encoding.\")\n",
    "    elif unique > 10:\n",
    "        print(f\"{column} has many unique categories â€” consider frequency encoding or feature grouping.\")\n",
    "    else:\n",
    "        print(f\"{column} seems low cardinality â€” standard encoding should work fine.\")\n",
    "\n",
    "    if (missing / total) > 0.05:\n",
    "        print(f\" Missing values exceed 5% â€” consider imputing or flagging missingness.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1c3a9a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# We can loop to save time\n",
    "for col in categorical_features:\n",
    "    univariate_categorical_eda(df1, col)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
