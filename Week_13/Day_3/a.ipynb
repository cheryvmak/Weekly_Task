{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76045c44",
   "metadata": {},
   "source": [
    "#### **1. Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460d48f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Core libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Preprocessing libraries\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# Statistical libraries\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore, skew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e73474",
   "metadata": {},
   "source": [
    " **2. Data ingestion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f15dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = ''\n",
    "df = pd.read_csv(url)\n",
    "df.head()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b4bdb2",
   "metadata": {},
   "source": [
    " **2. Preliminary Descriptive Analysis(PDA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6591a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Rows: {df.shape[0]:,}\")\n",
    "print(f\"Columns: {df.shape[1]}\")\n",
    "df.info()\n",
    "df.dtypes\n",
    "df.duplicated().sum(axis=0)\n",
    "df = df.drop_duplicates()\n",
    "df.describe().T\n",
    "df.quality.unique()\n",
    "\n",
    "df['quality_label'] = df['quality'].map({3:'worst',4:'bad',5:'good',6:'good',7:'best',8:'best',9:'best'})\n",
    "for column in df:\n",
    "    print(f\"\\nSkewness: {df[column].skew():.3f}\")\n",
    "    print(f\"Kurtosis: {df[column].kurt():.3f}\")\n",
    "\n",
    "  # Frequency + proportion\n",
    "    freq = df[column].value_counts()\n",
    "    prop = df[column].value_counts(normalize=True) * 100\n",
    "    summary = pd.DataFrame({'Count': freq, 'Percentage': prop.round(2)})\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b85166",
   "metadata": {},
   "source": [
    "#### **3. Log-Transform Skewed Variables (EDA Recommendation)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568416b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-transform skewed variables as recommended by EDA\n",
    "print(\"=== LOG-TRANSFORMING SKEWED VARIABLES ===\")\n",
    "print(\"EDA identified these variables as right-skewed and recommended log transformation:\")\n",
    "\n",
    "# Variables to log-transform based on EDA findings\n",
    "skewed_vars = ['x', 'y', 'z']\n",
    "\n",
    "for var in skewed_vars:\n",
    "    if var in df.columns:\n",
    "        # Check if variable has zero or negative values\n",
    "        min_val = df[var].min()\n",
    "        if min_val <= 0:\n",
    "            # Use log1p for variables with zeros\n",
    "            df[f'{var}_log'] = np.log1p(df[var])\n",
    "            print(f\"✓ {var}: Applied log1p transformation (had {min_val:.3f} minimum value)\")\n",
    "        else:\n",
    "            # Use log for positive values only\n",
    "            df[f'{var}_log'] = np.log(df[var])\n",
    "            print(f\"✓ {var}: Applied log transformation\")\n",
    "        \n",
    "        # Check skewness before and after\n",
    "        original_skew = skew(df[var])\n",
    "        transformed_skew = skew(df[f'{var}_log'])\n",
    "        print(f\"  Original skewness: {original_skew:.3f} → Transformed skewness: {transformed_skew:.3f}\")\n",
    "\n",
    "print(f\"\\nDataset shape after log transformation: {df.shape}\")\n",
    "print(\"New log-transformed columns:\", [col for col in df.columns if '_log' in col])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8105dd5b",
   "metadata": {},
   "source": [
    "#### **4. Outlier Treatment (EDA Recommendation)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce570c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier treatment based on EDA recommendations\n",
    "print(\"=== OUTLIER TREATMENT (IQR-CAPPING METHOD) ===\")\n",
    "print(\"EDA recommended IQR-capping method for outlier treatment.\\n\")\n",
    "\n",
    "# Define numerical columns (excluding target)\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'quality' in numerical_cols:\n",
    "    numerical_cols.remove('quality')\n",
    "\n",
    "print(f\"Treating outliers in {len(numerical_cols)} numerical features...\")\n",
    "\n",
    "# Apply IQR-capping method\n",
    "outliers_capped = 0\n",
    "for col in numerical_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Count outliers before capping\n",
    "    outliers_before = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
    "    \n",
    "    if outliers_before > 0:\n",
    "        # Cap outliers\n",
    "        df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])\n",
    "        df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])\n",
    "        outliers_capped += outliers_before\n",
    "        print(f\"✓ {col}: Capped {outliers_before} outliers\")\n",
    "\n",
    "print(f\"\\nTotal outliers capped: {outliers_capped}\")\n",
    "print(f\"Dataset shape after outlier treatment: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee58732",
   "metadata": {},
   "source": [
    "### 5. Encoding(like label encoding and one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f74d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "        'has_job_experience': {'Y': 1, 'N': 0},\n",
    "        'requires_job_training': {'Y': 1, 'N': 0},\n",
    "        'full_time_position': {'Y': 1, 'N': 0},\n",
    "        'case_status': {'Certified': 1, 'Denied': 0}\n",
    "    }\n",
    "\n",
    "for col, mapping in label_map.items():\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].map(mapping)\n",
    "\n",
    "    # One-hot Encoding (multi-category columns)\n",
    "        onehot_cols = [\n",
    "        'continent',\n",
    "        'education_of_employee',\n",
    "        'region_of_employment',\n",
    "        'unit_of_wage'\n",
    "    ]\n",
    "\n",
    "df = pd.get_dummies(df, columns=onehot_cols, drop_first=False, dtype=int)\n",
    "df\n",
    "\n",
    "print(\"Transformation (encoding) complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c638328",
   "metadata": {},
   "source": [
    "### 6. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7249af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== STRATIFIED DATA SPLITTING ===\")\n",
    "print(\"EDA identified class imbalance - using stratified splitting to preserve class distribution\")\n",
    "\n",
    "# Split features and target\n",
    "# 1. Define the feature variables (X) and target variable (y)\n",
    "X = df.drop('Risk', axis=1)  # Features (drop the target column)\n",
    "Y = df['Risk']  # Target (Risk)\n",
    "\n",
    "X = pd.get_dummies(X, drop_first=True)  ## Complete the code to create dummies for X\n",
    "\n",
    "\n",
    "print(f\"Selected features shape: {X.shape}\")\n",
    "print(f\"Target variable: '{Y}'\")\n",
    "print(f\"Unique class distribution:\\n{y.value_counts(normalize=True).round(3)}\")\n",
    "\n",
    "# ---------- First Split (80% train+val, 20% test) ----------\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ---------- Second Split (75% train, 25% val of 80%) ----------\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# ---------- Summary ----------\n",
    "print(\"\\n=== DATA SPLIT RESULTS ===\")\n",
    "print(f\"Training set:   {X_train.shape} ({(X_train.shape[0] / len(X)) * 100:.1f}%)\")\n",
    "print(f\"Validation set: {X_val.shape} ({(X_val.shape[0] / len(X)) * 100:.1f}%)\")\n",
    "print(f\"Test set:       {X_test.shape} ({(X_test.shape[0] / len(X)) * 100:.1f}%)\")\n",
    "\n",
    "# ---------- Class Distribution Verification ----------\n",
    "print(\"\\n=== CLASS DISTRIBUTION CHECK ===\")\n",
    "print(\"Training set case_status distribution:\")\n",
    "print(y_train.value_counts(normalize=True).round(3))\n",
    "print(\"\\nValidation set case_status distribution:\")\n",
    "print(y_val.value_counts(normalize=True).round(3))\n",
    "print(\"\\nTest set case_status distribution:\")\n",
    "print(y_test.value_counts(normalize=True).round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f79a49b",
   "metadata": {},
   "source": [
    "### 7. Feature Scaling (Normalization / Standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aabb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== FEATURE SCALING (STANDARD SCALER) ===\")\n",
    "print(\"EDA recommended StandardScaler for distance-based models\\n\")\n",
    "\n",
    "# Replace NaN or inf values before scaling\n",
    "X_train = X_train.replace([np.inf, -np.inf], np.nan).fillna(X_train.median())\n",
    "X_val = X_val.replace([np.inf, -np.inf], np.nan).fillna(X_val.median())\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(X_test.median())\n",
    "\n",
    "# Initialize scaler and fit on training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "\n",
    "# Transform validation and test sets\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns, index=X_val.index)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "# Report summary stats\n",
    "print(\"✓ Scaling applied successfully!\")\n",
    "print(f\"Training set scaled - Mean: {X_train_scaled.mean().mean():.4f}, Std: {X_train_scaled.std().mean():.4f}\")\n",
    "print(f\"Validation set scaled - Mean: {X_val_scaled.mean().mean():.4f}, Std: {X_val_scaled.std().mean():.4f}\")\n",
    "print(f\"Test set scaled - Mean: {X_test_scaled.mean().mean():.4f}, Std: {X_test_scaled.std().mean():.4f}\")\n",
    "\n",
    "# Verify scaling behavior\n",
    "print(f\"\\nScaling verification:\")\n",
    "print(f\"Training set - Mean ≈ 0: {abs(X_train_scaled.mean().mean()) < 0.01}\")\n",
    "print(f\"Training set - Std ≈ 1: {abs(X_train_scaled.std().mean() - 1) < 0.01}\")\n",
    "\n",
    "print(\"\\nFeature scaling complete.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c033da",
   "metadata": {},
   "source": [
    "### 8. Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fe7280",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SAVING PREPROCESSED DATA \")\n",
    "\n",
    "# Save scaled datasets\n",
    "X_train_scaled.to_csv('X_train_scaled.csv', index=False)\n",
    "X_val_scaled.to_csv('X_val_scaled.csv', index=False)\n",
    "X_test_scaled.to_csv('X_test_scaled.csv', index=False)\n",
    "\n",
    "# Save target variables\n",
    "y_train.to_csv('y_train.csv', index=False)\n",
    "y_val.to_csv('y_val.csv', index=False)\n",
    "y_test.to_csv('y_test.csv', index=False)\n",
    "\n",
    "# Save preprocessing objects\n",
    "import joblib\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "\n",
    "# Save preprocessing summary\n",
    "preprocessing_summary = {\n",
    "    'original_shape': df.shape,\n",
    "    'final_shape': df.shape,\n",
    "    'train_samples': X_train_scaled.shape[0],\n",
    "    'val_samples': X_val_scaled.shape[0],\n",
    "    'test_samples': X_test_scaled.shape[0],\n",
    "    'scaling_method': 'StandardScaler',\n",
    "    'outlier_treatment': 'IQR_capping',\n",
    "    'log_transformed': ['x_log', 'y_log', 'z_log']\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('preprocessing_summary.json', 'w') as f:\n",
    "    json.dump(preprocessing_summary, f, indent=2)\n",
    "\n",
    "print(\"- Preprocessed data saved successfully!\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"- X_train_scaled.csv, X_val_scaled.csv, X_test_scaled.csv\")\n",
    "print(\"- y_train.csv, y_val.csv, y_test.csv\")\n",
    "print(\"- scaler.pkl\")\n",
    "print(\"- preprocessing_summary.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72de1c40",
   "metadata": {},
   "source": [
    "#### **9. Preprocessing Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f642889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final preprocessing summary\n",
    "print(\".....PREPROCESSING SUMMARY BASED ON EDA INSIGHTS.....\")\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(f\"Final processed dataset shape: {df.shape}\")\n",
    "print(f\"Training samples: {X_train_scaled.shape[0]}\")\n",
    "print(f\"Validation samples: {X_val_scaled.shape[0]}\")\n",
    "print(f\"Test samples: {X_test_scaled.shape[0]}\")\n",
    "\n",
    "print(\"\\n.....PREPROCESSING STEPS COMPLETED (EDA-BASED).....\")\n",
    "print(\"> Data quality assessment (no missing values, duplicates handled)\")\n",
    "print(\"> Log-transformation of skewed variables (x, y, z)\")\n",
    "print(\"> Outlier treatment using IQR-capping method\")\n",
    "print(\"> Stratified data splitting (preserves class distribution)\")\n",
    "print(\"> StandardScaler applied (EDA recommendation for distance-based models)\")\n",
    "print(\"> Data export (ready for modeling)\")\n",
    "print(\"\\n.....EDA EVIDENCE IMPLEMENTED.....\")\n",
    "print(\"> Skewed variables log-transformed as recommended\")\n",
    "print(f\"\\n Preprocessing completed successfully!!!!!!\")\n",
    "print(\".....Ready for modeling phase with EDA-informed preprocessing\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
